{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUWeYIfl6h8C"
   },
   "source": [
    "# Genereative Adversarial Networks\n",
    "\n",
    "The main goal of **Generative Adversarial Network** (GAN) is to generate images that are similar (but not identical) to training dataset.\n",
    "\n",
    "GAN consists of two neural networks that are trained against each other:\n",
    "\n",
    " * **Generator** takes a random vector, and should generate an image from it\n",
    " * **Discriminator** is a networks that should distinguish between original image (from training dataset), and the one generated by the generator.\n",
    "\n",
    "<img src=\"./images/gan_architecture.png\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:20:11.559876Z",
     "iopub.status.busy": "2022-04-10T01:20:11.559606Z",
     "iopub.status.idle": "2022-04-10T01:20:11.566476Z",
     "shell.execute_reply": "2022-04-10T01:20:11.564054Z",
     "shell.execute_reply.started": "2022-04-10T01:20:11.559847Z"
    },
    "id": "vh9JiNcw80sd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:20:12.588294Z",
     "iopub.status.busy": "2022-04-10T01:20:12.588042Z",
     "iopub.status.idle": "2022-04-10T01:20:12.593031Z",
     "shell.execute_reply": "2022-04-10T01:20:12.592025Z",
     "shell.execute_reply.started": "2022-04-10T01:20:12.588267Z"
    },
    "id": "DglVjX5inb6t"
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "train_size = 1.0\n",
    "lr = 2e-4\n",
    "weight_decay = 8e-9\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "batch_size = 256\n",
    "epochs = 100\n",
    "plot_every = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlNHFbgyJ0t-"
   },
   "source": [
    "## Generator\n",
    "\n",
    "The role of a generator is to take a random vector of some size (it is similar to latent vector in autoencoders) and generate the target image. It is very similar to the generative side of autoencoder.\n",
    "\n",
    "In our example, we will use linear neural networks and MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:20:14.069449Z",
     "iopub.status.busy": "2022-04-10T01:20:14.068899Z",
     "iopub.status.idle": "2022-04-10T01:20:14.078472Z",
     "shell.execute_reply": "2022-04-10T01:20:14.077547Z",
     "shell.execute_reply.started": "2022-04-10T01:20:14.069411Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(100, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256, momentum=0.2)\n",
    "        self.linear2 = nn.Linear(256, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512, momentum=0.2)\n",
    "        self.linear3 = nn.Linear(512, 1024)\n",
    "        self.bn3 = nn.BatchNorm1d(1024, momentum=0.2)\n",
    "        self.linear4 = nn.Linear(1024, 784)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden1 = self.leaky_relu(self.bn1(self.linear1(input)))\n",
    "        hidden2 = self.leaky_relu(self.bn2(self.linear2(hidden1)))\n",
    "        hidden3 = self.leaky_relu(self.bn3(self.linear3(hidden2)))\n",
    "        generated = self.tanh(self.linear4(hidden3)).view(input.shape[0], 1, 28, 28)\n",
    "        return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ktsWKcUGrJh"
   },
   "source": [
    "A few tricks used in generator:\n",
    "* Instead of ReLU, we use **LeakyReLU**, i.e. a ReLU which is not exactly 0 for negative $x$, but rather another linear function with very small slope.\n",
    "* We use **BatchNorm1D** in order to stabilize training\n",
    "* The activation function on last layer is **Tanh**, so the output is in the range [-1,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKbYa6ABKDlt"
   },
   "source": [
    "## Discriminator\n",
    "\n",
    "Discriminator is a classical image classification network. In our first example, we will also use linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:20:16.009537Z",
     "iopub.status.busy": "2022-04-10T01:20:16.008735Z",
     "iopub.status.idle": "2022-04-10T01:20:16.016511Z",
     "shell.execute_reply": "2022-04-10T01:20:16.015828Z",
     "shell.execute_reply.started": "2022-04-10T01:20:16.009497Z"
    },
    "id": "JXTCDQvtl2q1"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(784, 512)\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.linear3 = nn.Linear(256, 1)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(input.shape[0], -1)\n",
    "        hidden1 = self.leaky_relu(self.linear1(input))\n",
    "        hidden2 = self.leaky_relu(self.linear2(hidden1))\n",
    "        classififed = self.sigmoid(self.linear3(hidden2))\n",
    "        return classififed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEKMvh3ljARS"
   },
   "source": [
    "## Loading dataset\n",
    "\n",
    "We will use MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:20:17.585727Z",
     "iopub.status.busy": "2022-04-10T01:20:17.585477Z",
     "iopub.status.idle": "2022-04-10T01:20:17.591292Z",
     "shell.execute_reply": "2022-04-10T01:20:17.590403Z",
     "shell.execute_reply.started": "2022-04-10T01:20:17.585698Z"
    },
    "id": "qzaNwIrYnXYu"
   },
   "outputs": [],
   "source": [
    "def mnist(train_part, transform=None):\n",
    "    dataset = torchvision.datasets.MNIST('.', download=True, transform=transform)\n",
    "    train_part = int(train_part * len(dataset))\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_part, len(dataset) - train_part])\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:20:18.391622Z",
     "iopub.status.busy": "2022-04-10T01:20:18.391368Z",
     "iopub.status.idle": "2022-04-10T01:20:18.396190Z",
     "shell.execute_reply": "2022-04-10T01:20:18.395457Z",
     "shell.execute_reply.started": "2022-04-10T01:20:18.391593Z"
    },
    "id": "_8cwsRK_uYaJ"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=0.5, std=0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:20:19.217944Z",
     "iopub.status.busy": "2022-04-10T01:20:19.217246Z",
     "iopub.status.idle": "2022-04-10T01:20:19.253572Z",
     "shell.execute_reply": "2022-04-10T01:20:19.252843Z",
     "shell.execute_reply.started": "2022-04-10T01:20:19.217904Z"
    },
    "id": "o_LQTueXnZmt"
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = mnist(train_size, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:20:20.096334Z",
     "iopub.status.busy": "2022-04-10T01:20:20.095652Z",
     "iopub.status.idle": "2022-04-10T01:20:20.101955Z",
     "shell.execute_reply": "2022-04-10T01:20:20.101119Z",
     "shell.execute_reply.started": "2022-04-10T01:20:20.096284Z"
    },
    "id": "X1FNaNmynex_"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, drop_last=True, batch_size=batch_size, shuffle=True)\n",
    "dataloaders = (train_dataloader, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhyAVbdMKWaq"
   },
   "source": [
    "## Network training\n",
    "\n",
    "On each step of the training, we have **two** phases:\n",
    "\n",
    "* **Generator** training. We generate some random vectors **noise** (training happens in minibatches, so we use 100 vectors at a time), generate **true labels** (vector with shape (bs, 1) with 1.0 values), calculate generator loss between output from **frozen** discriminator with noise as input and true labels.\n",
    "\n",
    "* **Discriminator** training. We calculate discriminator loss from **two** parts, **first** part is loss between output from discriminator with noise as input and **fake labels** (vector with shape (bs, 1) with 0.0 values), **second** part is loss between output from discriminator with real images as input and true labels (vector with shape (bs, 1) with 1.0 values). **Result loss** is (first_part_loss + second_part_loss) / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:20:21.587504Z",
     "iopub.status.busy": "2022-04-10T01:20:21.586841Z",
     "iopub.status.idle": "2022-04-10T01:20:21.593000Z",
     "shell.execute_reply": "2022-04-10T01:20:21.592212Z",
     "shell.execute_reply.started": "2022-04-10T01:20:21.587469Z"
    },
    "id": "aISx_wo4W1qJ"
   },
   "outputs": [],
   "source": [
    "def plotn(n, generator, device):\n",
    "    generator.eval()\n",
    "    noise = torch.FloatTensor(np.random.normal(0, 1, (n, 100))).to(device)\n",
    "    imgs = generator(noise).detach().cpu()\n",
    "    fig, ax = plt.subplots(1, n)\n",
    "    for i, im in enumerate(imgs):\n",
    "        ax[i].imshow(im[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:20:22.530305Z",
     "iopub.status.busy": "2022-04-10T01:20:22.529754Z",
     "iopub.status.idle": "2022-04-10T01:20:22.544480Z",
     "shell.execute_reply": "2022-04-10T01:20:22.543805Z",
     "shell.execute_reply.started": "2022-04-10T01:20:22.530268Z"
    },
    "id": "tlgpI9Dz-wB3"
   },
   "outputs": [],
   "source": [
    "def train_gan(dataloaders, models, optimizers, loss_fn, epochs, plot_every, device):\n",
    "    tqdm_iter = tqdm(range(epochs))\n",
    "    train_dataloader = dataloaders[0]\n",
    "    \n",
    "    gen, disc = models[0], models[1]\n",
    "    optim_gen, optim_disc = optimizers[0], optimizers[1]\n",
    "\n",
    "    for epoch in tqdm_iter:\n",
    "        gen.train()\n",
    "        disc.train()\n",
    "\n",
    "        train_gen_loss = 0.0\n",
    "        train_disc_loss = 0.0\n",
    "        \n",
    "        test_gen_loss = 0.0\n",
    "        test_disc_loss = 0.0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            imgs, _ = batch\n",
    "            imgs = imgs.to(device)\n",
    "\n",
    "            disc.eval()\n",
    "            gen.zero_grad()\n",
    "\n",
    "            noise = torch.FloatTensor(np.random.normal(0.0, 1.0, (imgs.shape[0], 100))).to(device)\n",
    "            real_labels = torch.ones((imgs.shape[0], 1)).to(device)\n",
    "            fake_labels = torch.zeros((imgs.shape[0], 1)).to(device)\n",
    "            \n",
    "            generated = gen(noise)\n",
    "            disc_preds = disc(generated)\n",
    "\n",
    "            g_loss = loss_fn(disc_preds, real_labels)\n",
    "            g_loss.backward()\n",
    "            optim_gen.step()\n",
    "\n",
    "            disc.train()\n",
    "            disc.zero_grad()\n",
    "\n",
    "            disc_real = disc(imgs)\n",
    "            disc_real_loss = loss_fn(disc_real, real_labels)\n",
    "\n",
    "            disc_fake = disc(generated.detach())\n",
    "            disc_fake_loss = loss_fn(disc_fake, fake_labels)\n",
    "\n",
    "            d_loss = (disc_real_loss + disc_fake_loss) / 2.0\n",
    "            d_loss.backward()\n",
    "            optim_disc.step()\n",
    "\n",
    "            train_gen_loss += g_loss.item()\n",
    "            train_disc_loss += d_loss.item()\n",
    "\n",
    "        train_gen_loss /= len(train_dataloader)\n",
    "        train_disc_loss /= len(train_dataloader)\n",
    "\n",
    "        if epoch % plot_every == 0 or epoch == epochs - 1:\n",
    "            plotn(5, gen, device)\n",
    "\n",
    "        tqdm_dct = {'generator loss:': train_gen_loss, 'discriminator loss:': train_disc_loss}\n",
    "        tqdm_iter.set_postfix(tqdm_dct, refresh=True)\n",
    "        tqdm_iter.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:20:23.478552Z",
     "iopub.status.busy": "2022-04-10T01:20:23.478289Z",
     "iopub.status.idle": "2022-04-10T01:20:23.505760Z",
     "shell.execute_reply": "2022-04-10T01:20:23.505120Z",
     "shell.execute_reply.started": "2022-04-10T01:20:23.478522Z"
    },
    "id": "AOx0xtgJtI68"
   },
   "outputs": [],
   "source": [
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "optimizer_generator = optim.Adam(generator.parameters(), lr=lr, weight_decay=weight_decay, betas=(beta1, beta2))\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=lr, weight_decay=weight_decay, betas=(beta1, beta2))\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "models = (generator, discriminator)\n",
    "optimizers = (optimizer_generator, optimizer_discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:20:24.424759Z",
     "iopub.status.busy": "2022-04-10T01:20:24.424155Z",
     "iopub.status.idle": "2022-04-10T01:35:56.984533Z",
     "shell.execute_reply": "2022-04-10T01:35:56.983850Z",
     "shell.execute_reply.started": "2022-04-10T01:20:24.424716Z"
    },
    "id": "7O1pMJPGuEpi"
   },
   "outputs": [],
   "source": [
    "train_gan(dataloaders, models, optimizers, loss_fn, epochs, plot_every, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-wH0b7pT0CI"
   },
   "source": [
    "## DCGAN\n",
    "\n",
    "**Deep Convolutional GAN** is pretty obvious idea of using convolutional layers for generator and discriminator. The main difference here is using **Conv2DTranspose** layer in the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dcgan_generator.png\" width=\"60%\"/>\n",
    "\n",
    "> Image from [this tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:35:56.987037Z",
     "iopub.status.busy": "2022-04-10T01:35:56.986233Z",
     "iopub.status.idle": "2022-04-10T01:35:56.998780Z",
     "shell.execute_reply": "2022-04-10T01:35:56.998137Z",
     "shell.execute_reply.started": "2022-04-10T01:35:56.986989Z"
    }
   },
   "outputs": [],
   "source": [
    "class DCGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.ConvTranspose2d(100, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.conv2 = nn.ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.ConvTranspose2d(64, 1, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), output_padding=(1, 1), bias=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden1 = self.relu(self.bn1(self.conv1(input)))\n",
    "        hidden2 = self.relu(self.bn2(self.conv2(hidden1)))\n",
    "        hidden3 = self.relu(self.bn3(self.conv3(hidden2)))\n",
    "        generated = self.tanh(self.conv4(hidden3)).view(input.shape[0], 1, 28, 28)\n",
    "        return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:35:57.000335Z",
     "iopub.status.busy": "2022-04-10T01:35:57.000077Z",
     "iopub.status.idle": "2022-04-10T01:35:57.012130Z",
     "shell.execute_reply": "2022-04-10T01:35:57.011312Z",
     "shell.execute_reply.started": "2022-04-10T01:35:57.000299Z"
    }
   },
   "outputs": [],
   "source": [
    "class DCDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden1 = self.leaky_relu(self.conv1(input))\n",
    "        hidden2 = self.leaky_relu(self.bn2(self.conv2(hidden1)))\n",
    "        hidden3 = self.leaky_relu(self.bn3(self.conv3(hidden2)))\n",
    "        classified = self.sigmoid(self.conv4(hidden3)).view(input.shape[0], -1)\n",
    "        return classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights initialization from [DCGAN](https://arxiv.org/pdf/1511.06434.pdf) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:35:57.014809Z",
     "iopub.status.busy": "2022-04-10T01:35:57.014104Z",
     "iopub.status.idle": "2022-04-10T01:35:57.022445Z",
     "shell.execute_reply": "2022-04-10T01:35:57.021645Z",
     "shell.execute_reply.started": "2022-04-10T01:35:57.014753Z"
    }
   },
   "outputs": [],
   "source": [
    "def weights_init(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(model.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(model.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(model.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:35:57.024139Z",
     "iopub.status.busy": "2022-04-10T01:35:57.023769Z",
     "iopub.status.idle": "2022-04-10T01:35:57.031121Z",
     "shell.execute_reply": "2022-04-10T01:35:57.030266Z",
     "shell.execute_reply.started": "2022-04-10T01:35:57.024103Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:35:57.033229Z",
     "iopub.status.busy": "2022-04-10T01:35:57.032411Z",
     "iopub.status.idle": "2022-04-10T01:35:57.070751Z",
     "shell.execute_reply": "2022-04-10T01:35:57.070031Z",
     "shell.execute_reply.started": "2022-04-10T01:35:57.033162Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = mnist(train_size, transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, drop_last=True, batch_size=batch_size, shuffle=True)\n",
    "dataloaders = (train_dataloader, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:35:57.072273Z",
     "iopub.status.busy": "2022-04-10T01:35:57.071962Z",
     "iopub.status.idle": "2022-04-10T01:35:57.095673Z",
     "shell.execute_reply": "2022-04-10T01:35:57.095017Z",
     "shell.execute_reply.started": "2022-04-10T01:35:57.072237Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = DCGenerator().to(device)\n",
    "generator.apply(weights_init)\n",
    "discriminator = DCDiscriminator().to(device)\n",
    "discriminator.apply(weights_init)\n",
    "optimizer_generator = optim.Adam(generator.parameters(), lr=lr, weight_decay=weight_decay, betas=(beta1, beta2))\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=lr, weight_decay=weight_decay, betas=(beta1, beta2))\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "models = (generator, discriminator)\n",
    "optimizers = (optimizer_generator, optimizer_discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:35:57.097248Z",
     "iopub.status.busy": "2022-04-10T01:35:57.096943Z",
     "iopub.status.idle": "2022-04-10T01:35:57.103766Z",
     "shell.execute_reply": "2022-04-10T01:35:57.103040Z",
     "shell.execute_reply.started": "2022-04-10T01:35:57.097210Z"
    }
   },
   "outputs": [],
   "source": [
    "def dcplotn(n, generator, device):\n",
    "    generator.eval()\n",
    "    noise = torch.FloatTensor(np.random.normal(0, 1, (n, 100, 1, 1))).to(device)\n",
    "    imgs = generator(noise).detach().cpu()\n",
    "    fig, ax = plt.subplots(1, n)\n",
    "    for i, im in enumerate(imgs):\n",
    "        ax[i].imshow(im[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:35:57.105734Z",
     "iopub.status.busy": "2022-04-10T01:35:57.105195Z",
     "iopub.status.idle": "2022-04-10T01:35:57.119565Z",
     "shell.execute_reply": "2022-04-10T01:35:57.118864Z",
     "shell.execute_reply.started": "2022-04-10T01:35:57.105668Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_dcgan(dataloaders, models, optimizers, loss_fn, epochs, plot_every, device):\n",
    "    tqdm_iter = tqdm(range(epochs))\n",
    "    train_dataloader = dataloaders[0]\n",
    "    \n",
    "    gen, disc = models[0], models[1]\n",
    "    optim_gen, optim_disc = optimizers[0], optimizers[1]\n",
    "    \n",
    "    gen.train()\n",
    "    disc.train()\n",
    "\n",
    "    for epoch in tqdm_iter:\n",
    "        train_gen_loss = 0.0\n",
    "        train_disc_loss = 0.0\n",
    "        \n",
    "        test_gen_loss = 0.0\n",
    "        test_disc_loss = 0.0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            imgs, _ = batch\n",
    "            imgs = imgs.to(device)\n",
    "            imgs = 2.0 * imgs - 1.0\n",
    "\n",
    "            gen.zero_grad()\n",
    "\n",
    "            noise = torch.FloatTensor(np.random.normal(0.0, 1.0, (imgs.shape[0], 100, 1, 1))).to(device)\n",
    "            real_labels = torch.ones((imgs.shape[0], 1)).to(device)\n",
    "            fake_labels = torch.zeros((imgs.shape[0], 1)).to(device)\n",
    "            \n",
    "            generated = gen(noise)\n",
    "            disc_preds = disc(generated)\n",
    "\n",
    "            g_loss = loss_fn(disc_preds, real_labels)\n",
    "            g_loss.backward()\n",
    "            optim_gen.step()\n",
    "\n",
    "            disc.zero_grad()\n",
    "\n",
    "            disc_real = disc(imgs)\n",
    "            disc_real_loss = loss_fn(disc_real, real_labels)\n",
    "\n",
    "            disc_fake = disc(generated.detach())\n",
    "            disc_fake_loss = loss_fn(disc_fake, fake_labels)\n",
    "\n",
    "            d_loss = (disc_real_loss + disc_fake_loss) / 2.0\n",
    "            d_loss.backward()\n",
    "            optim_disc.step()\n",
    "\n",
    "            train_gen_loss += g_loss.item()\n",
    "            train_disc_loss += d_loss.item()\n",
    "\n",
    "        train_gen_loss /= len(train_dataloader)\n",
    "        train_disc_loss /= len(train_dataloader)\n",
    "\n",
    "        if epoch % plot_every == 0 or epoch == epochs - 1:\n",
    "            dcplotn(5, gen, device)\n",
    "\n",
    "        tqdm_dct = {'generator loss:': train_gen_loss, 'discriminator loss:': train_disc_loss}\n",
    "        tqdm_iter.set_postfix(tqdm_dct, refresh=True)\n",
    "        tqdm_iter.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:35:57.122490Z",
     "iopub.status.busy": "2022-04-10T01:35:57.122172Z",
     "iopub.status.idle": "2022-04-10T01:44:51.314525Z",
     "shell.execute_reply": "2022-04-10T01:44:51.313819Z",
     "shell.execute_reply.started": "2022-04-10T01:35:57.122459Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dcgan(dataloaders, models, optimizers, loss_fn, epochs // 2, plot_every // 2, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T01:46:05.504296Z",
     "iopub.status.busy": "2022-04-10T01:46:05.503731Z",
     "iopub.status.idle": "2022-04-10T01:46:05.889844Z",
     "shell.execute_reply": "2022-04-10T01:46:05.889135Z",
     "shell.execute_reply.started": "2022-04-10T01:46:05.504257Z"
    }
   },
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "dcplotn(5, generator, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqkGxSk8mXFr"
   },
   "source": [
    "> **Task**: Try generating more complex color images with DCGAN - for example, take one class from [CIFAR-10](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wc8rDu7uk98o"
   },
   "source": [
    "## Training on Paintings\n",
    "\n",
    "One of the good candidates for GAN training are paintings created by human artists.\n",
    "\n",
    "![](https://soshnikov.com/images/artartificial/Flo1.jpg)\n",
    "\n",
    "(Photo from [Art of Artificial](https://soshnikov.com/museum/art-artificial/) collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_A3nR10Tkih"
   },
   "source": [
    "## References\n",
    "\n",
    "* [GAN arxiv](https://arxiv.org/abs/1406.2661)\n",
    "* [DCGAN arxiv](https://arxiv.org/abs/1511.06434)\n",
    "* [DCGAN Pytorch tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
