{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUWeYIfl6h8C"
   },
   "source": [
    "\n",
    "# Genereative Adversarial Networks\n",
    "\n",
    "The main goal of **Generative Adversarial Network** (GAN) is to generate images that are similar (but not identical) to training dataset.\n",
    "\n",
    "GAN consists of two neural networks that are trained against each other:\n",
    "\n",
    " * **Generator** takes a random vector, and should generate an image from it\n",
    " * **Discriminator** is a networks that should distinguish between original image (from training dataset), and the one generated by the generator.\n",
    "\n",
    "<img src=\"./images/gan_architecture.png\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vh9JiNcw80sd",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlNHFbgyJ0t-"
   },
   "source": [
    "## Generator\n",
    "\n",
    "The role of a generator is to take a random vector of some size (it is similar to latent vector in autoencoders) and generate the target image. It is very similar to the generative side of autoencoder.\n",
    "\n",
    "In our example, we will use dense neural networks and MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOWm55Bd5DNl",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "generator = Sequential()\n",
    "generator.add(Dense(256, input_shape=(100,)))\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "generator.add(Dense(512))\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "generator.add(Dense(1024))\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "generator.add(Dense(784, activation='tanh'))\n",
    "generator.add(Reshape((28,28)))\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.0002, decay=8e-9)\n",
    "\n",
    "generator.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKbYa6ABKDlt"
   },
   "source": [
    "A few tricks used in generator:\n",
    "* Instead of ReLU, we use **Leaky ReLU**, i.e. a ReLU which is not exactly 0 for negative $x$, but rather another linear function with very small slope. This is important, because it helps gradient descent to propagate values even if we are on the negative side of ReLU (where values are 0)\n",
    "* We use Batch Normalization in order to stabilize training\n",
    "* The activation function on last layer is `tanh`, so the output is in the range [-1,1]\n",
    "\n",
    "## Discriminator\n",
    "\n",
    "Discriminator is a classical image classification network. In our first example, we will also use dense classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krxyG2oh87pU",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "discriminator = Sequential()\n",
    "discriminator.add(Flatten(input_shape=(28,28)))\n",
    "discriminator.add(Dense(784))\n",
    "\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dense(784//2))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "discriminator.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "discriminator.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kH2ox9EIKO8F"
   },
   "source": [
    "We will also define an adversarial network, which is generator followed by discriminator. This network starts with a noise vector, and returns a binary result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsJH1VWQ9yeB",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "discriminator.trainable = False\n",
    "adversarial = Sequential()\n",
    "adversarial.add(generator)\n",
    "adversarial.add(discriminator)\n",
    "adversarial.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset\n",
    "\n",
    "We will use MNIST dataset, built into Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzX0kMrW-dQA",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "(X_train, _), (_, _) = keras.datasets.mnist.load_data()\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhyAVbdMKWaq"
   },
   "source": [
    "## Network training\n",
    "\n",
    "On each step of the training, we have two phases:\n",
    "\n",
    "* Training discriminator:\n",
    "   - We generate some random vectors `noise`. Training happens in minibatches, so we use `batch//2` vectors to produce `batch//2` generated images\n",
    "   - Sample `batch//2` random images from the dataset\n",
    "   - Train discriminator on 50% real and 50% generated images, providing corresponding labels (0 or 1)\n",
    " * Train the generator by using combined adversarial model, passing random vectors as input, and expecting 1's as output (which corresponds to real images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aISx_wo4W1qJ",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def plotn(n):\n",
    "  noise = np.random.normal(0, 1, (n,100))\n",
    "  imgs = generator.predict(noise)\n",
    "  fig,ax = plt.subplots(1,n)\n",
    "  for i,im in enumerate(imgs):\n",
    "    ax[i].imshow(im.reshape(28,28))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 673
    },
    "id": "mNdcpEa3_Pqs",
    "outputId": "75ebec09-e608-4747-8298-0cb2281d95af",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "batch=32\n",
    "for cnt in range(3000):\n",
    "## train discriminator\n",
    "  random_index =  np.random.randint(0, len(X_train) - batch//2)\n",
    "  legit_images = X_train[random_index : random_index + batch//2].reshape(batch//2, 28, 28)\n",
    "  gen_noise = np.random.normal(0, 1, (batch//2,100))\n",
    "  syntetic_images = generator.predict(gen_noise)\n",
    "  x_combined_batch = np.concatenate((legit_images, syntetic_images))\n",
    "  y_combined_batch = np.concatenate((np.ones((batch//2, 1)), np.zeros((batch//2, 1))))\n",
    "  d_loss = discriminator.train_on_batch(x_combined_batch, y_combined_batch)\n",
    "# train generator\n",
    "  noise = np.random.normal(0, 1, (batch,100))\n",
    "  y_mislabled = np.ones((batch, 1))\n",
    "  g_loss = adversarial.train_on_batch(noise, y_mislabled)\n",
    "  if cnt%500==0:\n",
    "    print ('epoch: %d, [Discriminator :: d_loss: %f], [ Generator :: loss: %f]' % (cnt, d_loss[0], g_loss))\n",
    "    plotn(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJH1n4wRkqPy"
   },
   "source": [
    "> **Task**: You can train this GAN on the whole MNIST dataset and see how good can it get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-wH0b7pT0CI"
   },
   "source": [
    "## DCGAN\n",
    "\n",
    "In the previous example, we have used dense networks for both generator and discriminator, but we know that CNNs provide better performance when dealing with images. **Deep Convolutional GAN** is similar to the architecture above, but it uses convolutional layers for generator and discriminator. \n",
    "\n",
    "The main difficulty here is to build an architecture for denerator, because it has to do an inverse task compared to traditional CNN - it has to generate image from feature vector. In a way, this is similar to decoder part of autoencoders.That's why we will be using `Conv2DTranspose` layers in the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fkD2WufskBdm",
    "outputId": "b4a22c66-a493-4abd-dc79-55c9fd7be137",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "(X_train, _), (_, _) = keras.datasets.mnist.load_data()\n",
    "X_train = (X_train.astype(np.float32)-127.5) / 127.5\n",
    "print(X_train.min(),X_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tAk-hsF0TzEv",
    "outputId": "68bba515-be2d-44f5-f079-b461456ed082",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "generator = Sequential()\n",
    "generator.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=100))\n",
    "generator.add(Reshape((7, 7, 128)))\n",
    "generator.add(UpSampling2D())\n",
    "generator.add(Conv2DTranspose(128, kernel_size=3, padding=\"same\"))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "generator.add(Activation(\"relu\"))\n",
    "generator.add(UpSampling2D())\n",
    "generator.add(Conv2DTranspose(64, kernel_size=3, padding=\"same\"))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "generator.add(Activation(\"relu\"))\n",
    "generator.add(Conv2DTranspose(1, kernel_size=3, padding=\"same\"))\n",
    "generator.add(Activation(\"tanh\"))\n",
    "\n",
    "optimizer = keras.optimizers.Adam(0.0001) #, 0.5)\n",
    "\n",
    "generator.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9FZHScTUsCd",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "discriminator = Sequential()\n",
    "\n",
    "discriminator.add(Conv2D(32, kernel_size=3, strides=2, input_shape=(28,28,1), padding=\"same\"))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(0.25))\n",
    "discriminator.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "discriminator.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "discriminator.add(BatchNormalization(momentum=0.8))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(0.25))\n",
    "discriminator.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "discriminator.add(BatchNormalization(momentum=0.8))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(0.25))\n",
    "discriminator.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "discriminator.add(BatchNormalization(momentum=0.8))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(0.25))\n",
    "discriminator.add(Flatten())\n",
    "discriminator.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "discriminator.compile(loss='binary_crossentropy',optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Jz166pQWRba",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "discriminator.trainable = False\n",
    "adversarial = Sequential()\n",
    "adversarial.add(generator)\n",
    "adversarial.add(discriminator)\n",
    "adversarial.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kohaucz-WTue",
    "outputId": "edf64874-5ec9-4c10-cb36-95e2bb5111cf",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "batch=32\n",
    "y_labeled = np.ones((batch, 1))\n",
    "y_mislabeled = np.zeros((batch, 1))\n",
    "for cnt in range(1000):\n",
    "## train discriminator\n",
    "  random_index =  np.random.randint(0, len(X_train) - batch)\n",
    "  legit_images = X_train[random_index : random_index + batch].reshape(batch, 28, 28, 1)\n",
    "  gen_noise = np.random.normal(0, 1, (batch,100))\n",
    "  syntetic_images = generator.predict(gen_noise)\n",
    "  d_loss_1 = discriminator.train_on_batch(legit_images, y_labeled)\n",
    "  d_loss_2 = discriminator.train_on_batch(syntetic_images, y_mislabeled)\n",
    "  d_loss = 0.5*np.add(d_loss_1,d_loss_2)\n",
    "# train generator\n",
    "  g_loss = adversarial.train_on_batch(gen_noise, y_labeled)\n",
    "  if cnt%100==0:\n",
    "    print ('epoch: %d, [Discriminator :: d_loss: %f], [ Generator :: loss: %f]' % (cnt, d_loss, g_loss))\n",
    "    plotn(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqkGxSk8mXFr"
   },
   "source": [
    "> **Task**: Try generating more complex color images with DCGAN - for example, take one class from [CIFAR-10](https://keras.io/api/datasets/cifar10/) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wc8rDu7uk98o"
   },
   "source": [
    "## Training on Paintings\n",
    "\n",
    "One of the good candidates for GAN training are paintings created by human artists. Below is a sample image produced by DCGAN trained on a dataset from [WikiArt](http://wikiart.org). [KeraGAN](http://github.com/shwars/keragan) library was used to produce this image [using Azure Machine Learning](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml/)\n",
    "\n",
    "![](https://soshnikov.com/images/artartificial/Flo1.jpg)\n",
    "\n",
    "(Photo from [Art of Artificial](https://soshnikov.com/museum/art-artificial/) collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_A3nR10Tkih"
   },
   "source": [
    "## References\n",
    "\n",
    "* [Keras implementation of different toy GAN architectures](https://github.com/eriklindernoren/Keras-GAN)\n",
    "* [KeraGAN Library](http://github.com/shwars/Keragan)\n",
    "* [Blog post about creating GANs on Azure ML](https://soshnikov.com/scienceart/creating-generative-art-using-gan-on-azureml-ru/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GANs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
